# MaxK SpGEMM Integration with GNN Training

This guide explains how to integrate the MaxK SpGEMM CUDA kernels with GNN training for accelerated performance.

## Overview

The integration provides:
- **MaxK SpGEMM Function**: Custom PyTorch autograd function using spmm_maxk.cu and spmm_maxk_backward.cu
- **MaxK SAGE Model**: Modified SAGE implementation with direct kernel calls
- **Fallback Support**: Automatic fallback to cuSPARSE/DGL when kernels are unavailable
- **Performance Monitoring**: Built-in timing and validation

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                    GNN Training Script                      │
│                 (maxk_gnn_integrated.py)                   │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│                MaxK SAGE Model                              │
│             (maxk_models_integrated.py)                     │
│  ┌─────────────────┬─────────────────┬─────────────────┐    │
│  │   MaxKSAGE     │ MaxKSAGEConv    │  MaxKSpmvWrapper│    │
│  └─────────────────┴─────────────────┴─────────────────┘    │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│              MaxK SpGEMM Function                           │
│            (maxk_spgemm_function.py)                        │
│  ┌─────────────────┬─────────────────┬─────────────────┐    │
│  │MaxKSpGEMMFunction│ AutoGrad Support│  Fallback Logic │    │
│  └─────────────────┴─────────────────┴─────────────────┘    │
└─────────────────────┬───────────────────────────────────────┘
                      │
┌─────────────────────▼───────────────────────────────────────┐
│              Direct CUDA Kernels                           │
│            (maxk_cuda_kernels.so)                          │
│  ┌─────────────────┬─────────────────┬─────────────────┐    │
│  │ spmm_maxk.cu   │spmm_maxk_back.cu│   cuSPARSE      │    │
│  │   (forward)    │   (backward)    │  (fallback)     │    │
│  └─────────────────┴─────────────────┴─────────────────┘    │
└─────────────────────────────────────────────────────────────┘
```

## Files Created

### Core Integration Files

1. **`maxk_spgemm_function.py`**
   - `MaxKSpGEMMFunction`: Custom autograd function
   - `MaxKSpmvWrapper`: Kernel management class
   - Automatic fallback logic

2. **`maxk_models_integrated.py`**
   - `MaxKSAGE`: Accelerated SAGE model
   - `MaxKSAGEConv`: Custom convolution layer
   - Compatibility with original models

3. **`maxk_gnn_integrated.py`**
   - Modified training script
   - Performance monitoring
   - Kernel validation

4. **`maxk_config_updated.py`**
   - Extended configuration options
   - MaxK kernel parameters
   - Validation settings

5. **`maxk_training_scripts.sh`**
   - Automated training scripts
   - Batch processing
   - Performance comparison

## Setup Instructions

### 1. Build MaxK Kernels

```bash
# Build the direct CUDA kernel bindings
python setup_direct_kernels.py build_ext --inplace

# Verify kernels are working
python -c "import maxk_cuda_kernels; print('✅ Kernels loaded')"
```

### 2. Generate Metadata

```bash
# Generate warp4 metadata for optimal kernel performance
cd kernels
python generate_meta.py
cd ..
```

### 3. Setup Training Environment

```bash
# Use the setup script
source maxk_training_scripts.sh
setup_maxk_training
```

## Usage Examples

### Basic Training with MaxK Acceleration

```python
# Import the integrated models
from maxk_models_integrated import MaxKSAGE

# Create model with graph name for metadata loading
model = MaxKSAGE(
    in_size=features.size(1),
    hid_size=256,
    num_hid_layers=3,
    out_size=num_classes,
    maxk=32,
    graph_name="reddit.dgl"  # For metadata loading
)

# The model automatically uses MaxK kernels when available
output = model(graph, features)
```

### Command Line Training

```bash
# Train with MaxK kernels
python maxk_gnn_integrated.py \
  --dataset reddit \
  --model sage \
  --maxk 32 \
  --use_maxk_kernels \
  --validate_kernels \
  --profile_kernels

# Compare performance: Original vs MaxK
source maxk_training_scripts.sh
compare_maxk_performance reddit 32 97 0
```

### Batch Training Multiple K Values

```bash
source maxk_training_scripts.sh
batch_train_maxk reddit sage 97 0
```

## Performance Monitoring

The integration includes built-in performance monitoring:

```
Epoch 0100: Avg Forward 8.234ms, Avg Backward 12.456ms
Final Timing - Avg Forward: 8.234ms, Avg Backward: 12.456ms
Total per epoch: 20.690ms
```

## Validation and Fallback

The system automatically validates kernel correctness:

```python
# Validation happens during first forward pass
✅ MaxK metadata loaded for reddit.dgl: 1250 warps
🔍 Validating MaxK kernel vs cuSPARSE for k=32
📊 Max error (at input nonzero): 0.00000123
✅ Validation PASSED! MaxK kernel produces correct results
```

If validation fails or kernels are unavailable:
```
⚠️ MaxK kernel failed: cuda error, falling back to cuSPARSE
⚠️ MaxK kernels not available, using cuSPARSE fallback
```

## Configuration Options

### Basic Options
```bash
--use_maxk_kernels          # Enable MaxK acceleration
--kernel_mode auto          # auto|maxk|cusparse|dgl
--validate_kernels          # Validate correctness
--profile_kernels           # Enable timing
```

### Advanced Options
```bash
--maxk_num_warps 12         # Number of warps
--maxk_warp_max_nz 64       # Max non-zeros per warp
--maxk_fallback_threshold 0.001  # Error threshold
--graph_metadata_path kernels/w12_nz64_warp_4/
```

## Expected Speedups

Based on the MaxK-GNN paper, for graphs with average degree > 50:

| k value | Speedup vs cuSPARSE | Speedup vs GNNAdvisor |
|---------|---------------------|------------------------|
| k=16    | 6.93x              | 9.57x                 |
| k=32    | 5.39x              | 7.46x                 |
| k=64    | 2.55x              | 3.55x                 |

## Troubleshooting

### Common Issues

1. **Kernels not building**
   ```bash
   # Check CUDA version compatibility
   nvcc --version
   python -c "import torch; print(torch.version.cuda)"
   
   # Rebuild with verbose output
   python setup_direct_kernels.py build_ext --inplace --force
   ```

2. **Metadata not found**
   ```bash
   # Generate metadata
   cd kernels
   python generate_meta.py
   ```

3. **Validation failures**
   ```bash
   # Test individual components
   python maxk_spgemm_function.py
   python maxk_models_integrated.py
   ```

4. **Performance not improving**
   - Check graph size (benefits increase with larger graphs)
   - Verify metadata is loaded correctly
   - Ensure k value is appropriate (16-64 typically optimal)

### Debug Mode

```bash
# Run with detailed debugging
python maxk_gnn_integrated.py \
  --dataset reddit \
  --model sage \
  --maxk 32 \
  --use_maxk_kernels \
  --validate_kernels \
  --profile_kernels \
  --epochs 10  # Short run for debugging
```

## Integration Benefits

1. **Performance**: 2-7x speedup over baseline implementations
2. **Memory Efficiency**: 90%+ reduction in memory traffic  
3. **Compatibility**: Seamless fallback to existing implementations
4. **Validation**: Built-in correctness checking
5. **Monitoring**: Detailed performance profiling

## Next Steps

1. **Extend to Other Models**: Integrate MaxK kernels with GCN, GIN
2. **Multi-GPU Support**: Extend to distributed training
3. **Dynamic K Selection**: Adaptive k values during training
4. **Kernel Fusion**: Combine multiple operations in single kernel

The integration provides a robust foundation for accelerated GNN training while maintaining compatibility with existing codebases.
